{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2. Vectorized thinking\n",
    "\n",
    "<div class=\"alert alert-warning\">**Goal of this notebook:**\n",
    "Understand some more of the systems context behind vectorized thinking.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized thinking is great for conciseness. Surely no one would prefer\n",
    "```\n",
    "assert len(xs) == len(ys)\n",
    "[xs[i] + ys[i] for i in range(len(xs))]\n",
    "```\n",
    "when they can just write\n",
    "```\n",
    "xs + ys\n",
    "```\n",
    "\n",
    "But vectorized coding is perhaps more important from the point of view of performance. Suppose `xs` and `ys` are both very large vectors. With list comprehension, it's almost impossible for the compiler to figure out that the operation can be parallelized, and each core of a multicore machine could be working on a chunk of the data.\n",
    "\n",
    "Here's another example. Suppose we have a vector `xs` where each entry is the contents of a web page, and the entire vector is 100 TB and it's sharded across machines, and we want to compute\n",
    "```\n",
    "m = -inf\n",
    "for x in xs:\n",
    "    m = max(m, f(x))\n",
    "```\n",
    "where `f` is some scoring function that is slow to evaluate. Again, it's hard for a compiler to see how to achieve parallelism when the function is written out as an explicit iteration. (Maybe not hard in this case, but hard in the general case!) But it's easy to see how the computation _could_ be distributed: tell each machine in the cluster to work on the fragment of data it can reach and compute the maximum over its fragment, then assemble the maximums from each machine. If we write the computation abstractly, e.g.\n",
    "```\n",
    "reduce(pairwise_max, -inf, map(f, xs))\n",
    "```\n",
    "then the compiler can figure out how to distribute it.\n",
    "\n",
    "Vectorized thinking means avoiding `for` loops and instead writing our computations in a way that shows our intention more clearly, to give the compiler a chance to figure out what can be distributed and parallelized."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
