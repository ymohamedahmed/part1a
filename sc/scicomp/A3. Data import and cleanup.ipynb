{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3. Data import and cleanup\n",
    "\n",
    "In my experience, around 75% of the time you spend working with data will be fighting to import it and clean it up. For the most part this is just general-purpose programming, but there are a few library routines that will save you from reinventing the wheel.\n",
    "\n",
    "* [A3.0 Preamble](#preamble)\n",
    "* [A3.1 Reading from a csv file](#readfile)\n",
    "* [A3.2 Reading from a url](#readurl)\n",
    "* [A3.3 Parsing a log file with regular expressions](#regexp)\n",
    "* [A3.4 Reading JSON from a web service](#json)\n",
    "* [A3.5 Scraping a website with xpath](#xpath)\n",
    "* [A3.6 Querying an SQL database](#sql)\n",
    "\n",
    "Treat this section as a collection of recipes and pointers to useful library routines. If you find yourself needing them, you should read the recipe, try it out, then look online for more information about the library functions it suggests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3.0 Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules, and give them short aliases so we can write e.g. np.foo rather than numpy.foo\n",
    "import math, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "# The next line is a piece of magic, to let plots appear in our Jupyter notebooks\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3.1 Reading from a csv file <span id=\"readfile\"></span>\n",
    "When your data is a very simple comma-separated value (CSV) file then it's very easy to import. A CSV file looks like this: a header line, then one line per row of the data frame, values separated by commas.\n",
    "```\n",
    "\"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\",\"Petal.Width\",\"Species\"\n",
    "5.1,3.5,1.4,0.2,\"setosa\"\n",
    "4.9,3,1.4,0.2,\"setosa\"\n",
    "4.7,3.2,1.3,0.2,\"setosa\"\n",
    "4.6,3.1,1.5,0.2,\"setosa\"\n",
    "5,3.6,1.4,0.2,\"setosa\"\n",
    "```\n",
    "Here is code to import a simple CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res/iris.csv') as f:\n",
    "    df = pandas.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `with` syntax is useful for files and similar objects which need to be closed/released after we've finished with them, to avoid memory leaks. It's equivalent to calling `f=open('data/iris.csv')`, then running the other commands, then calling `f.close()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your file is nearly a CSV but has some quirks such as comments or a missing header row, experiment with the options in [`pandas.read_csv`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) or [`pandas.read_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html). For extreme quirks you may need to use the raw Python [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3.2 Reading from a URL or other file-like object<span id=\"readurl\"></span>\n",
    "You can use the same `pandas.read_csv` function to read any file-like thing, such as a files retrieved over the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request  # standard library for web requests\n",
    "my_url = \"https://teachingfiles.blob.core.windows.net/scicomp/iris.csv\"\n",
    "with urllib.request.urlopen(my_url) as f:\n",
    "    df = pandas.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also read from a string as if it were a file using [`io.StringIO`](https://docs.python.org/3/library/io.html#io.StringIO). This is sometimes handy for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "f = io.StringIO('''\n",
    "x,y\n",
    "10,3\n",
    "2,5\n",
    "''')\n",
    "df = pandas.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3.3 Parsing a text log file<span id=\"regexp\"></span>\n",
    "A typical line from a web server log might look like this\n",
    "```\n",
    "207.46.13.169 - - [27/Aug/2017:06:52:11 +0000] \"GET /marcus/essay/st&h2.html HTTP/1.1\" 200 3881 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X) AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"\n",
    "```\n",
    "where (according to the [Apache web server documentation](https://httpd.apache.org/docs/2.4/logs.html#combined)) the pieces are\n",
    "\n",
    "* **`207.46.13.169`** \n",
    "The IP address that made the request\n",
    "* **`-`** \n",
    "The identity of the client; `-` means not available\n",
    "* **`-`**\n",
    "The userid of the logged-in user who made the request; `-` means not available\n",
    "* **`[27/Aug/2017:06:52:11 +0000]`**\n",
    "The time the request was received\n",
    "* **`\"GET /marcus/essay/st&h2.html HTTP/1.1\"`**\n",
    "The type of request, what was requested, and the connection type\n",
    "* **`200`**\n",
    "The [http status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) (200 means OK)\n",
    "* **`3881`**\n",
    "The size of the object returned to the client, in bytes\n",
    "* **`-`**\n",
    "The referrer URL; `-` means not available\n",
    "* **`\"Mozilla/5.0 (...)\"`**\n",
    "The browser type. The substring `bingbot` here tells us that the request comes from Microsoft Bing's web crawler.\n",
    "\n",
    "To extract these pieces from a single line of the log file, the best tool is [regular expressions](https://docs.python.org/3.4/library/re.html), a mini-language for string matching that is common across many programming languages. The syntax is terse and takes a lot of practice. I like to start with a small string pattern and incrementally build it up, testing as I go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 16), match='\\n207.46.13.169 -'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re    # standard Python module for regular expressions\n",
    "s = \"\"\"\n",
    "207.46.13.169 - - [27/Aug/2017:06:52:11 +0000] \"GET /marcus/essay/st&h2.html HTTP/1.1\" \n",
    "200 3881 \"-\" \"Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X)\n",
    "AppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53\n",
    "(compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)\"\n",
    "\"\"\"\n",
    "\n",
    "# First attempt: match the first two items in the log line.\n",
    "# If my pattern is right, re.match returns an object.\n",
    "# If my pattern is wrong, re.match returns None.\n",
    "pattern_test = r'\\s*(\\S+)\\s*(\\S+)'\n",
    "re.match(pattern_test, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'client': '-',\n",
       " 'ip': '207.46.13.169',\n",
       " 'ref': '-',\n",
       " 'req': 'GET /marcus/essay/st&h2.html HTTP/1.1',\n",
       " 'size': '3881',\n",
       " 'status': '200',\n",
       " 't': '27/Aug/2017:06:52:11 +0000',\n",
       " 'ua': 'Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Mac OS X)\\nAppleWebKit/537.51.1 (KHTML, like Gecko) Version/7.0 Mobile/11A465 Safari/9537.53\\n(compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)',\n",
       " 'user': '-'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the full pattern I built up to. Python lets us add verbose comments\n",
    "# to the pattern, which is handy for remembering what your code does when you look\n",
    "# at it the next morning.\n",
    "pattern = r'''(?x)  #   flag saying that this pattern has comments\n",
    "\\s*                 #   any whitespace at the start of the string\n",
    "(?P<ip>\\S+)         # one or more non-space characters: the IP address\n",
    "\\s+                 #   one or more spaces\n",
    "(?P<client>\\S+)     # the client identity\n",
    "\\s+\n",
    "(?P<user>\\S+)       # the userid\n",
    "\\s+\n",
    "\\[(?P<t>[^\\]]*)\\]   # [, then any number of not-] characters, then ]: the timestamp\n",
    "\\s+\n",
    "\"(?P<req>[^\"]*)\"    # \", then any number of not-\" characters, then \": the request string\n",
    "\\s+\n",
    "(?P<status>\\d+)     # one or more numerical digits: the http status code\n",
    "\\s+\n",
    "(?P<size>\\d+)       # one or more numerical digits: the size\n",
    "\\s+\n",
    "\"(?P<ref>[^\"]*)\"    # the referrer URL\n",
    "\\s+\n",
    "\"(?P<ua>[^\"]*)\"     # the user agent i.e. browser type\n",
    "'''\n",
    "m = re.match(pattern, s)\n",
    "m.groupdict()       # returns a dictionary of all the named sub-patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "**Exercise.** Why not use <code style=\"background-color:inherit\">s.split(' ')</code>?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we extract the fields from a full log file? The standard Python code is\n",
    "```\n",
    "with open(myfile) as f:\n",
    "    for line in f:\n",
    "        m = re.match(pattern, line)\n",
    "        # store the fields from m.groups() or m.groupdict() somewhere appropriate\n",
    "```\n",
    "Alternatively, `numpy` has a handy shortcut for reading in an entire file and splitting it via a regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client</th>\n",
       "      <th>ip</th>\n",
       "      <th>ref</th>\n",
       "      <th>req</th>\n",
       "      <th>size</th>\n",
       "      <th>status</th>\n",
       "      <th>t</th>\n",
       "      <th>ua</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>207.46.13.169</td>\n",
       "      <td>-</td>\n",
       "      <td>GET /marcus/essay/st&amp;h2.html HTTP/1.1</td>\n",
       "      <td>3881</td>\n",
       "      <td>200</td>\n",
       "      <td>27/Aug/2017:06:52:11 +0000</td>\n",
       "      <td>Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Ma...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>31.218.127.114</td>\n",
       "      <td>https://www.google.ae/</td>\n",
       "      <td>GET /damon/recipe/oventemperatures.html HTTP/1.1</td>\n",
       "      <td>879</td>\n",
       "      <td>200</td>\n",
       "      <td>27/Aug/2017:06:52:43 +0000</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.3; 5050X Build/J...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-</td>\n",
       "      <td>31.218.127.114</td>\n",
       "      <td>http://www.wischik.com/damon/recipe/oventemper...</td>\n",
       "      <td>GET /damon/recipe/recipe.css HTTP/1.1</td>\n",
       "      <td>1118</td>\n",
       "      <td>200</td>\n",
       "      <td>27/Aug/2017:06:52:43 +0000</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.3; 5050X Build/J...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-</td>\n",
       "      <td>31.218.127.114</td>\n",
       "      <td>http://www.wischik.com/damon/recipe/oventemper...</td>\n",
       "      <td>GET /favicon.ico HTTP/1.1</td>\n",
       "      <td>503</td>\n",
       "      <td>404</td>\n",
       "      <td>27/Aug/2017:06:52:44 +0000</td>\n",
       "      <td>Mozilla/5.0 (Linux; Android 4.3; 5050X Build/J...</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  client              ip                                                ref  \\\n",
       "0      -   207.46.13.169                                                  -   \n",
       "1      -  31.218.127.114                             https://www.google.ae/   \n",
       "2      -  31.218.127.114  http://www.wischik.com/damon/recipe/oventemper...   \n",
       "3      -  31.218.127.114  http://www.wischik.com/damon/recipe/oventemper...   \n",
       "\n",
       "                                                req  size status  \\\n",
       "0             GET /marcus/essay/st&h2.html HTTP/1.1  3881    200   \n",
       "1  GET /damon/recipe/oventemperatures.html HTTP/1.1   879    200   \n",
       "2             GET /damon/recipe/recipe.css HTTP/1.1  1118    200   \n",
       "3                         GET /favicon.ico HTTP/1.1   503    404   \n",
       "\n",
       "                            t  \\\n",
       "0  27/Aug/2017:06:52:11 +0000   \n",
       "1  27/Aug/2017:06:52:43 +0000   \n",
       "2  27/Aug/2017:06:52:43 +0000   \n",
       "3  27/Aug/2017:06:52:44 +0000   \n",
       "\n",
       "                                                  ua user  \n",
       "0  Mozilla/5.0 (iPhone; CPU iPhone OS 7_0 like Ma...    -  \n",
       "1  Mozilla/5.0 (Linux; Android 4.3; 5050X Build/J...    -  \n",
       "2  Mozilla/5.0 (Linux; Android 4.3; 5050X Build/J...    -  \n",
       "3  Mozilla/5.0 (Linux; Android 4.3; 5050X Build/J...    -  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the file into an array, one row per line, one column per field in the pattern\n",
    "df = np.fromregex('res/webaccess_short.log', pattern, dtype=np.unicode_)\n",
    "# Make a dictionary out of the columns, according to the named fields in the pattern\n",
    "df = pandas.DataFrame({v: df[:,i-1] for v,i in re.compile(pattern).groupindex.items()})\n",
    "\n",
    "df[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3.4 Reading json from a web data service<span id=\"json\"></span>\n",
    "More and more forward-thinking companies and government services make data available by simple web requests. Here is an example, importing river levels from the UK's [real-time flood monitoring API](http://environment.data.gov.uk/flood-monitoring/doc/reference).\n",
    "\n",
    "The first step is to import the Python module for making web requests. When I'm developing data code I like to build it up in small steps, which means lots of repeated requests, so I also like to use another Python module which caches responses. This means I don't hammer the service unnecessarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, requests_cache\n",
    "requests_cache.install_cache('floodsystem', backend='memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [flood monitoring API documentation](http://environment.data.gov.uk/flood-monitoring/doc/reference) tells us the URL for fetching a list of all stations, `{root}/id/stations`. Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make the request. This should print out <Response [200]>, meaning successfully retrieved\n",
    "stations_resp = requests.get('http://environment.data.gov.uk/flood-monitoring/id/stations')\n",
    "stations_resp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the body of the response. It's likely to be very long, so we'll only print out the first 300 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{ \\n  \"@context\" : \"http://environment.data.gov.uk/flood-monitoring/meta/context.jsonld\" ,\\n  \"meta\" : { \\n    \"publisher\" : \"Environment Agency\" ,\\n    \"licence\" : \"http://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/\" ,\\n    \"documentation\" : \"http://environment.data.gov.uk/flood-m'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stations_resp.text[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like [JSON](https://en.wikipedia.org/wiki/JSON), \"JavaScript Object Notation\", a common format for web data services. It's easy to turn it it into Python dictionaries and lists, and to explore what it contains. (Alternatively, just read the web service documentation, if you trust it!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "dict_keys(['@context', 'meta', 'items'])\n",
      "<class 'list'>\n",
      "4346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'@id': 'http://environment.data.gov.uk/flood-monitoring/id/stations/1029TH',\n",
       " 'RLOIid': '7041',\n",
       " 'catchmentName': 'Cotswolds',\n",
       " 'dateOpened': '1994-01-01',\n",
       " 'easting': 417990,\n",
       " 'label': 'Bourton Dickler',\n",
       " 'lat': 51.874767,\n",
       " 'long': -1.740083,\n",
       " 'measures': [{'@id': 'http://environment.data.gov.uk/flood-monitoring/id/measures/1029TH-level-downstage-i-15_min-mASD',\n",
       "   'parameter': 'level',\n",
       "   'parameterName': 'Water Level',\n",
       "   'period': 900,\n",
       "   'qualifier': 'Downstream Stage',\n",
       "   'unitName': 'mASD'},\n",
       "  {'@id': 'http://environment.data.gov.uk/flood-monitoring/id/measures/1029TH-level-stage-i-15_min-mASD',\n",
       "   'parameter': 'level',\n",
       "   'parameterName': 'Water Level',\n",
       "   'period': 900,\n",
       "   'qualifier': 'Stage',\n",
       "   'unitName': 'mASD'}],\n",
       " 'northing': 219610,\n",
       " 'notation': '1029TH',\n",
       " 'riverName': 'Dikler',\n",
       " 'stageScale': 'http://environment.data.gov.uk/flood-monitoring/id/stations/1029TH/stageScale',\n",
       " 'stationReference': '1029TH',\n",
       " 'status': 'http://environment.data.gov.uk/flood-monitoring/def/core/statusActive',\n",
       " 'town': 'Little Rissington',\n",
       " 'wiskiID': '1029TH'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = stations_resp.json()\n",
    "print(x.__class__)                        # The top-level response is a dictionary\n",
    "print(x.keys())                           # What are its keys? 'items' sounds promising.\n",
    "print(x['items'].__class__)               # 'items' is a list,\n",
    "print(len(x['items']))                    # with 4363 elements.\n",
    "\n",
    "stations = stations_resp.json()['items']  # This is what we're really after\n",
    "stations[0]                               # Look at a single sensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data from web services is often patchy and inconsistent, so your code for processing it should be full of error handling. For example,\n",
    "```\n",
    "rivers = [s['riverName'] for s in stations]\n",
    "```\n",
    "will fail because `riverName` isn't present for all stations. You might use one of these instead:\n",
    "```\n",
    "rivers = [s.get('riverName', None) for s in stations]\n",
    "rivers = [s['riverName'] for s in stations if 'riverName' in s]\n",
    "```\n",
    "Here's how we might turn the data into a data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.DataFrame({\n",
    "    'label': np.array([s['label'] for s in stations]),\n",
    "    'riverName': np.array([s.get('riverName',None) for s in stations]),\n",
    "    'town': np.array([s.get('town',None) for s in stations])\n",
    "})\n",
    "\n",
    "# Each station has zero or more measures. Pick out the first 'water level' measure at each.\n",
    "# The field measure['@id'] is the url to use to get water level readings.\n",
    "measures = [[m for m in s.get('measures',{}) if m['parameter']=='level'] for s in stations]\n",
    "df['url'] = np.array([ms[0]['@id'] if len(ms)>0 else None for ms in measures])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3.5 Scraping a website with xpath<span id=\"xpath\"></span>\n",
    "There are fascinating stories to be discovered from public data, and sometimes you have to work to scrape it from web pages. Here's [an acount](https://onlinejournalismblog.com/2016/11/29/how-the-bbc-england-data-unit-scraped-airport-noise-complaints/) by a BBC data journalist. We'll work with a very simple example: extracting results of the Oxford / Cambridge boat race from the [Wikipedia table](https://en.wikipedia.org/wiki/List_of_The_Boat_Race_results#Main_race).\n",
    "\n",
    "I recommend using [XPath queries](https://www.w3.org/TR/xpath20/) from the [`lxml`](http://lxml.de/) module.\n",
    "XPath is a powerful mini-language for extracting data from hierarchical documents, with wide support across many programming languages &mdash; think of it as regular expressions but for html rather than plain text. If you want to scrape websites then it's worth finding a tutorial and learning XPath. For this course, we'll just see how to use XPath in Python.\n",
    "\n",
    "The first step is to install `lxml`, which is not included with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fetch the web page and parse the contents. Most web pages are badly-formatted html (sections not properly closed, etc.), and `lxml.html.fromstring` makes a reasonable attempt to make sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import lxml.html\n",
    "boatrace_url = 'https://en.wikipedia.org/wiki/List_of_The_Boat_Race_results'\n",
    "resp = requests.get(boatrace_url)\n",
    "doc = lxml.html.fromstring(resp.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us `doc`, the root `<html>` element, which we can inspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "html\n",
      "2\n",
      "['head', 'body']\n",
      "{'class': 'client-nojs', 'lang': 'en', 'dir': 'ltr'}\n",
      "\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "print(doc.tag)               # the type of element\n",
    "print(len(doc))              # the number of children\n",
    "print([n.tag for n in doc])  # tags of its children, <head> and <body>\n",
    "print(doc.attrib)            # get the attributes, e.g. <html class=\"client-nojs\" lang=\"en\" dir=\"ltr\">\n",
    "print(doc.text, doc.tail)    # any text directly under in this element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to pull out a particular element from the document, namely the table with boat race results, so we need to work out how to refer to it in XPath. The Chrome webbrowser has a handy tool to help with this. Go to the page you're interested in, and click on View | Developer | Developer Tools. Click on the element-selector button at the top left:\n",
    "<img src=\"res/xpath1.png\" alt=\"use 'select element' mode\">\n",
    "Go back to the web page, and click on a piece close to what you want to select. I clicked on the top left cell of the table:\n",
    "<img src=\"res/xpath2.png\" alt=\"click roughly where you want\" style=\"height:8em\">\n",
    "Go back to the developer tools window, and navigate to the exact element you want. Here, we want the table. Right-click and choose Copy | Copy XPath. \n",
    "<img src=\"res/xpath3.png\" alt=\"copy the XPath of the element you want\" style=\"height:8em\">\n",
    "It gave me the XPath location `\"//*[@id=\"mw-content-text\"]/div/table[2]\"`. Now we can extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tr>\n",
      "<td align=\"center\"><a href=\"/wiki/The_Boat_Race_1829\" title=\"The Boat Race 1829\">1</a></td>\n",
      "<td align=\"center\"><span class=\"sortkey\" style=\"display:none;speak:none\">000000001829-06-10-0000</span><span style=\"white-space:nowrap\">10 June 1829</span> <img alt=\"dagger\" src=\"//upload.wikimedia.org/wikipedia/commons/3/37/Dagger-14-plain.png\" width=\"9\" height=\"14\" data-file-width=\"9\" data-file-height=\"14\"/><br/>\n",
      "<span style=\"color:red;font-size:85%\">1830â€“1835 no race</span></td>\n",
      "<td style=\"background:#004685; color:#FFFFFF\" align=\"center\">Oxford</td>\n",
      "<td align=\"center\">14:03</td>\n",
      "<td align=\"center\"><span style=\"display:none\">999</span>Easily</td>\n",
      "<td align=\"center\">1</td>\n",
      "<td align=\"center\">0</td>\n",
      "</tr>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t</th>\n",
       "      <th>winner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000001829-06-10-0000</td>\n",
       "      <td>Oxford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000001836-06-17-0000</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000001839-04-03-0000</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000001840-04-15-0000</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000001841-04-14-0000</td>\n",
       "      <td>Cambridge</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         t     winner\n",
       "0  000000001829-06-10-0000     Oxford\n",
       "1  000000001836-06-17-0000  Cambridge\n",
       "2  000000001839-04-03-0000  Cambridge\n",
       "3  000000001840-04-15-0000  Cambridge\n",
       "4  000000001841-04-14-0000  Cambridge"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pick out the table element.\n",
    "# (XPath queries return lists, but I only want one item, hence the [0].)\n",
    "table = doc.xpath('//*[@id=\"mw-content-text\"]/div/table[2]')[0]\n",
    "\n",
    "# Get a list of all rows i.e. <tr> elements inside the table.\n",
    "# Print one, to check things look OK.\n",
    "rows = table.xpath('.//tr')\n",
    "print(lxml.etree.tostring(rows[1], encoding='unicode'))\n",
    "\n",
    "# Extract the timestamp and winner columns.\n",
    "# The timestamp is in the second child, in a <span> element with class \"sortkey\".\n",
    "# The winner is in the third child.\n",
    "df = {'t': [row[1].xpath('.//span[contains(@class, \"sortkey\")]')[0].text for row in rows[1:]],\n",
    "      'winner': [row[2].text for row in rows[1:]]}\n",
    "df = pandas.DataFrame(df)\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should consider the ethics of your web scraping. Here are some thoughts:\n",
    "from [Sophie Chou at the MIT Media Lab](http://www.storybench.org/to-scrape-or-not-to-scrape-the-technical-and-ethical-challenges-of-collecting-data-off-the-web/), and the [data journalist N&auml;el Shiab](https://gijn.org/2015/08/12/on-the-ethics-of-web-scraping-and-data-journalism/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3.6 Querying an SQL database<span id=\"sql\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once your data is in an SQL database, access is easy. Here's an example with Postgresql, a dialect of SQL. I don't like to include secret credentials in an Azure notebook. Instead, I store the credentials in a file, and I make sure the file isn't cloned in a public repository. The file looks something like\n",
    "\n",
    "```\n",
    "{\"user\": \"***\", \"password\": \"***\", \"host\": \"***\", \"dbname\": \"***\"}\n",
    "```\n",
    "so that Python can load it in as a simple dictionary. The syntax `connect(**creds)` passes each item in the dictionary as a named argument, i.e. `connect(user=\"***\", ...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2    # module for connecting to a Postgresql database\n",
    "import json        # standard module for reading json files\n",
    "\n",
    "creds = json.load(open('res/secret_creds.json'))\n",
    "conn = psycopg2.connect(**creds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run arbitrary SQL queries. Pass in parameters with the `%(name)s` quoting mechanism, to keep yourself safe from SQL injection attacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>uri</th>\n",
       "      <th>label</th>\n",
       "      <th>id</th>\n",
       "      <th>catchment</th>\n",
       "      <th>river</th>\n",
       "      <th>town</th>\n",
       "      <th>lat</th>\n",
       "      <th>lng</th>\n",
       "      <th>index</th>\n",
       "      <th>measure_id</th>\n",
       "      <th>uri</th>\n",
       "      <th>station_uri</th>\n",
       "      <th>qualifier</th>\n",
       "      <th>parameter</th>\n",
       "      <th>period</th>\n",
       "      <th>unit</th>\n",
       "      <th>valuetype</th>\n",
       "      <th>low</th>\n",
       "      <th>high</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>345</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Great Chesterford</td>\n",
       "      <td>E21778</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Great Chesterford</td>\n",
       "      <td>52.061730</td>\n",
       "      <td>0.194279</td>\n",
       "      <td>397</td>\n",
       "      <td>398</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>m</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>800</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Weston Bampfylde</td>\n",
       "      <td>52113</td>\n",
       "      <td>Parrett, Brue and West Somerset Streams</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Weston Bampfylde</td>\n",
       "      <td>51.023159</td>\n",
       "      <td>-2.565568</td>\n",
       "      <td>918</td>\n",
       "      <td>919</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>m</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1272</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Cambridge Baits Bite</td>\n",
       "      <td>E60101</td>\n",
       "      <td>Cam and Ely Ouse (Including South Level)</td>\n",
       "      <td>River Cam</td>\n",
       "      <td>Milton</td>\n",
       "      <td>52.236542</td>\n",
       "      <td>0.176925</td>\n",
       "      <td>1454</td>\n",
       "      <td>1455</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>http://environment.data.gov.uk/flood-monitorin...</td>\n",
       "      <td>Stage</td>\n",
       "      <td>Water Level</td>\n",
       "      <td>900.0</td>\n",
       "      <td>mASD</td>\n",
       "      <td>instantaneous</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                                uri  \\\n",
       "0    345  http://environment.data.gov.uk/flood-monitorin...   \n",
       "1    800  http://environment.data.gov.uk/flood-monitorin...   \n",
       "2   1272  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                  label      id                                 catchment  \\\n",
       "0     Great Chesterford  E21778  Cam and Ely Ouse (Including South Level)   \n",
       "1      Weston Bampfylde   52113   Parrett, Brue and West Somerset Streams   \n",
       "2  Cambridge Baits Bite  E60101  Cam and Ely Ouse (Including South Level)   \n",
       "\n",
       "       river               town        lat       lng  index  measure_id  \\\n",
       "0  River Cam  Great Chesterford  52.061730  0.194279    397         398   \n",
       "1  River Cam   Weston Bampfylde  51.023159 -2.565568    918         919   \n",
       "2  River Cam             Milton  52.236542  0.176925   1454        1455   \n",
       "\n",
       "                                                 uri  \\\n",
       "0  http://environment.data.gov.uk/flood-monitorin...   \n",
       "1  http://environment.data.gov.uk/flood-monitorin...   \n",
       "2  http://environment.data.gov.uk/flood-monitorin...   \n",
       "\n",
       "                                         station_uri qualifier    parameter  \\\n",
       "0  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "1  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "2  http://environment.data.gov.uk/flood-monitorin...     Stage  Water Level   \n",
       "\n",
       "   period  unit      valuetype    low   high  \n",
       "0   900.0     m  instantaneous  0.109  0.333  \n",
       "1   900.0     m  instantaneous  0.026  0.600  \n",
       "2   900.0  mASD  instantaneous  0.218  0.294  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = '''\n",
    "SELECT *\n",
    "FROM flood_stations AS s JOIN flood_measures AS m ON (m.station_uri = s.uri) \n",
    "WHERE river = %(river)s OR town = %(town)s\n",
    "LIMIT 3\n",
    "'''\n",
    "pandas.read_sql(cmd, conn, params={'river': 'River Cam', 'town': 'Cambridge'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
